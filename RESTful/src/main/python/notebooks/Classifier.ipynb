{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mattia/DGA-Test\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=0.9)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_style({'font.family':'monospace'})\n",
    "import os\n",
    "import ntpath\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import statistics \n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import scipy.stats as scstat            \n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "from unidecode import unidecode\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import numbers\n",
    "from collections.abc import MutableMapping\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import requests,json,psutil,datetime\n",
    "import timeit\n",
    "\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# If true exports vectorial PDFs instead of JPG.\n",
    "VECTORIAL_FIGURES = False\n",
    "FIG_EXTENSION = \"pdf\" if VECTORIAL_FIGURES else \"jpg\"\n",
    "\n",
    "ROOT_DIR = \"/home/mattia/DGA-Test/\"\n",
    "DATA_DIR = ROOT_DIR + \"Data/\"\n",
    "MODELS_DIR = ROOT_DIR + \"Models/\"\n",
    "GRAPHICS_DIR = ROOT_DIR + \"Graphics/\" + FIG_EXTENSION + \"/\"\n",
    "\n",
    "# Change path to root\n",
    "os.chdir(ROOT_DIR)\n",
    "print(os.getcwd())\n",
    "\n",
    "try:\n",
    "    os.makedirs(GRAPHICS_DIR)\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(MODELS_DIR)\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "        \n",
    "from scipy.io import arff\n",
    "from resource import getrusage as resource_usage, RUSAGE_SELF\n",
    "from time import time as timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_replaced = {\n",
    "    'kraken': ['kraken_v1', 'kraken_v2'],\n",
    "    'CL01': ['alureon', 'fobber_v2'],\n",
    "    'CL02': ['gozi_gpl','gozi_luther', 'gozi_nasa', 'gozi_rfc4343', 'rovnix'],\n",
    "    'CL03': ['pykspa_noise', 'pykspa','proslikefan', 'tempedreve', 'qadars'],\n",
    "    'CL04': ['vawtrak_v2', 'vawtrak_v3'],\n",
    "    'CL05': ['pizd', 'suppobox_1'],\n",
    "    'CL06': ['dircrypt', 'bedep', 'ramnit'],\n",
    "    'CL07': ['ranbyus_v1', 'fobber_v1', 'cryptolocker'],\n",
    "    'CL08': ['ranbyus_v2', 'murofet_v2'],\n",
    "    'CL09': ['qakbot', 'murofet_v1'],\n",
    "    'CL10': ['matsnu', 'nymaim'],\n",
    "    'CL11': ['locky', 'necurs'],\n",
    "    'CL12': ['chinad', 'shiotob'],\n",
    "    'CL13': ['CL06', 'CL07', 'CL08', 'CL11'],\n",
    "    'CL14': ['CL03', 'vawtrak_v1', 'tinba'],\n",
    "    'CL15': ['CL09', 'CL13', 'CL14'],\n",
    "    'CL16': ['CL01', 'CL15', 'kraken'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Data Shape: (506810, 131)\n",
      "CPU times: user 38.2 s, sys: 1.16 s, total: 39.3 s\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = arff.loadarff(DATA_DIR + \"M10K-000.arff\")\n",
    "features = pd.DataFrame(data[0])\n",
    "#features = pd.read_csv(DATA_DIR + \"M10K-PCA.arff\", low_memory=False)\n",
    "\n",
    "features['class'] = features['class'].str.decode('utf-8')\n",
    "features.dropna(inplace=True)\n",
    "\n",
    "print('Complete Data Shape:', features.shape)\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['shiotob', 'gozi_luther', 'bedep', 'tempedreve', 'ramnit',\n",
       "       'kraken_v2', 'pykspa_noise', 'padcrypt', 'rovnix', 'qakbot',\n",
       "       'simda', 'corebot', 'banjori', 'vawtrak_v3', 'suppobox_3',\n",
       "       'ranbyus_v2', 'zeus-newgoz', 'fobber_v2', 'dircrypt', 'suppobox_1',\n",
       "       'sisron', 'murofet_v3', 'pushdo', 'ccleaner', 'locky',\n",
       "       'cryptolocker', 'symmi', 'dyre', 'fobber_v1', 'vawtrak_v1',\n",
       "       'legit', 'alureon', 'gozi_nasa', 'tinba', 'matsnu', 'chinad',\n",
       "       'proslikefan', 'murofet_v2', 'kraken_v1', 'pizd', 'necurs',\n",
       "       'gozi_rfc4343', 'murofet_v1', 'qadars', 'pykspa', 'suppobox_2',\n",
       "       'ranbyus_v1', 'nymaim', 'gozi_gpl', 'vawtrak_v2', 'ramdo'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.06 s, sys: 43.8 ms, total: 9.11 s\n",
      "Wall time: 9.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Replace the classes\n",
    "for key,values in to_be_replaced.items():\n",
    "    for value in values:\n",
    "        features['class'] = features['class'].str.replace(pat=value, repl=key, regex=False)\n",
    "        \n",
    "# Rebalance the dataset to remove random data from the replaced classes\n",
    "features = features.sample(frac=1, random_state=42).groupby(by=['class']).head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 209628 entries, 98509 to 122254\n",
      "Data columns (total 131 columns):\n",
      "nlp_1g_25p        float64\n",
      "nlp_1g_50p        float64\n",
      "nlp_1g_75p        float64\n",
      "nlp_1g_cov        float64\n",
      "nlp_1g_dist       float64\n",
      "nlp_1g_dst_ca     float64\n",
      "nlp_1g_dst_ch     float64\n",
      "nlp_1g_dst_em     float64\n",
      "nlp_1g_dst_eu     float64\n",
      "nlp_1g_dst_ji     float64\n",
      "nlp_1g_dst_kl     float64\n",
      "nlp_1g_dst_ma     float64\n",
      "nlp_1g_e          float64\n",
      "nlp_1g_ken        float64\n",
      "nlp_1g_kur        float64\n",
      "nlp_1g_mean       float64\n",
      "nlp_1g_norm       float64\n",
      "nlp_1g_pea        float64\n",
      "nlp_1g_pro        float64\n",
      "nlp_1g_pstd       float64\n",
      "nlp_1g_pvar       float64\n",
      "nlp_1g_qmean      float64\n",
      "nlp_1g_rep        float64\n",
      "nlp_1g_ske        float64\n",
      "nlp_1g_spe        float64\n",
      "nlp_1g_std        float64\n",
      "nlp_1g_sumsq      float64\n",
      "nlp_1g_tkur       float64\n",
      "nlp_1g_tpstd      float64\n",
      "nlp_1g_tpvar      float64\n",
      "nlp_1g_tske       float64\n",
      "nlp_1g_tstd       float64\n",
      "nlp_1g_tsum       float64\n",
      "nlp_1g_tsumsq     float64\n",
      "nlp_1g_tvar       float64\n",
      "nlp_1g_var        float64\n",
      "nlp_2g_25p        float64\n",
      "nlp_2g_50p        float64\n",
      "nlp_2g_75p        float64\n",
      "nlp_2g_cov        float64\n",
      "nlp_2g_dist       float64\n",
      "nlp_2g_dst_ca     float64\n",
      "nlp_2g_dst_ch     float64\n",
      "nlp_2g_dst_em     float64\n",
      "nlp_2g_dst_eu     float64\n",
      "nlp_2g_dst_ji     float64\n",
      "nlp_2g_dst_kl     float64\n",
      "nlp_2g_dst_ma     float64\n",
      "nlp_2g_e          float64\n",
      "nlp_2g_ken        float64\n",
      "nlp_2g_kur        float64\n",
      "nlp_2g_mean       float64\n",
      "nlp_2g_norm       float64\n",
      "nlp_2g_pea        float64\n",
      "nlp_2g_pro        float64\n",
      "nlp_2g_pstd       float64\n",
      "nlp_2g_pvar       float64\n",
      "nlp_2g_qmean      float64\n",
      "nlp_2g_rep        float64\n",
      "nlp_2g_ske        float64\n",
      "nlp_2g_spe        float64\n",
      "nlp_2g_std        float64\n",
      "nlp_2g_sumsq      float64\n",
      "nlp_2g_tkur       float64\n",
      "nlp_2g_tpstd      float64\n",
      "nlp_2g_tpvar      float64\n",
      "nlp_2g_tske       float64\n",
      "nlp_2g_tstd       float64\n",
      "nlp_2g_tsum       float64\n",
      "nlp_2g_tsumsq     float64\n",
      "nlp_2g_tvar       float64\n",
      "nlp_2g_var        float64\n",
      "nlp_3g_25p        float64\n",
      "nlp_3g_50p        float64\n",
      "nlp_3g_75p        float64\n",
      "nlp_3g_cov        float64\n",
      "nlp_3g_dist       float64\n",
      "nlp_3g_dst_ca     float64\n",
      "nlp_3g_dst_ch     float64\n",
      "nlp_3g_dst_em     float64\n",
      "nlp_3g_dst_eu     float64\n",
      "nlp_3g_dst_ji     float64\n",
      "nlp_3g_dst_kl     float64\n",
      "nlp_3g_dst_ma     float64\n",
      "nlp_3g_e          float64\n",
      "nlp_3g_ken        float64\n",
      "nlp_3g_kur        float64\n",
      "nlp_3g_mean       float64\n",
      "nlp_3g_norm       float64\n",
      "nlp_3g_pea        float64\n",
      "nlp_3g_pro        float64\n",
      "nlp_3g_pstd       float64\n",
      "nlp_3g_pvar       float64\n",
      "nlp_3g_qmean      float64\n",
      "nlp_3g_rep        float64\n",
      "nlp_3g_ske        float64\n",
      "nlp_3g_spe        float64\n",
      "nlp_3g_std        float64\n",
      "nlp_3g_sumsq      float64\n",
      "nlp_3g_tkur       float64\n",
      "nlp_3g_tpstd      float64\n",
      "nlp_3g_tpvar      float64\n",
      "nlp_3g_tske       float64\n",
      "nlp_3g_tstd       float64\n",
      "nlp_3g_tsum       float64\n",
      "nlp_3g_tsumsq     float64\n",
      "nlp_3g_tvar       float64\n",
      "nlp_3g_var        float64\n",
      "nlp_l_2dn         float64\n",
      "nlp_l_fqdn        float64\n",
      "nlp_l_odn         float64\n",
      "nlp_lc_c          float64\n",
      "nlp_lc_d          float64\n",
      "nlp_lc_v          float64\n",
      "nlp_n             float64\n",
      "nlp_r_con_2dn     float64\n",
      "nlp_r_con_fqdn    float64\n",
      "nlp_r_con_odn     float64\n",
      "nlp_r_let_2dn     float64\n",
      "nlp_r_let_fqdn    float64\n",
      "nlp_r_let_odn     float64\n",
      "nlp_r_num_2dn     float64\n",
      "nlp_r_num_fqdn    float64\n",
      "nlp_r_num_odn     float64\n",
      "nlp_r_sym_2dn     float64\n",
      "nlp_r_sym_fqdn    float64\n",
      "nlp_r_sym_odn     float64\n",
      "nlp_r_vow_2dn     float64\n",
      "nlp_r_vow_fqdn    float64\n",
      "nlp_r_vow_odn     float64\n",
      "class             object\n",
      "dtypes: float64(130), object(1)\n",
      "memory usage: 211.1+ MB\n"
     ]
    }
   ],
   "source": [
    "features.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CL02', 'CL04', 'CL05', 'CL10', 'CL12', 'CL16', 'banjori', 'ccleaner',\n",
       "       'corebot', 'dyre', 'legit', 'murofet_v3', 'padcrypt', 'pushdo', 'ramdo',\n",
       "       'simda', 'sisron', 'suppobox_2', 'suppobox_3', 'symmi', 'zeus-newgoz'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = features['class'].astype('category')\n",
    "features.drop('class', inplace=True, axis=1) \n",
    "display(labels.cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 209628 entries, 98509 to 122254\n",
      "Data columns (total 10 columns):\n",
      "nlp_l_2dn         209628 non-null float64\n",
      "nlp_r_con_2dn     209628 non-null float64\n",
      "nlp_r_let_fqdn    209628 non-null float64\n",
      "nlp_1g_norm       209628 non-null float64\n",
      "nlp_1g_tpstd      209628 non-null float64\n",
      "nlp_2g_dst_em     209628 non-null float64\n",
      "nlp_2g_tpstd      209628 non-null float64\n",
      "nlp_3g_dst_em     209628 non-null float64\n",
      "nlp_3g_dst_eu     209628 non-null float64\n",
      "nlp_3g_norm       209628 non-null float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 17.6 MB\n"
     ]
    }
   ],
   "source": [
    "features_to_keep = [\"nlp_l_2dn\", \n",
    "                    \"nlp_r_con_2dn\", \n",
    "                    \"nlp_r_let_fqdn\",\n",
    "                    \"nlp_1g_norm\",\n",
    "                    \"nlp_1g_tpstd\",\n",
    "                    \"nlp_2g_dst_em\",\n",
    "                    \"nlp_2g_tpstd\",\n",
    "                    \"nlp_3g_dst_em\",\n",
    "                    \"nlp_3g_dst_eu\",\n",
    "                    \"nlp_3g_norm\"\n",
    "                   ]\n",
    "\n",
    "features = features[features_to_keep]\n",
    "features.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'CL02',\n",
       " 1: 'CL04',\n",
       " 2: 'CL05',\n",
       " 3: 'CL10',\n",
       " 4: 'CL12',\n",
       " 5: 'CL16',\n",
       " 6: 'banjori',\n",
       " 7: 'ccleaner',\n",
       " 8: 'corebot',\n",
       " 9: 'dyre',\n",
       " 10: 'legit',\n",
       " 11: 'murofet_v3',\n",
       " 12: 'padcrypt',\n",
       " 13: 'pushdo',\n",
       " 14: 'ramdo',\n",
       " 15: 'simda',\n",
       " 16: 'sisron',\n",
       " 17: 'suppobox_2',\n",
       " 18: 'suppobox_3',\n",
       " 19: 'symmi',\n",
       " 20: 'zeus-newgoz'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_map = dict( enumerate(labels.cat.categories ) )\n",
    "for k,v in category_map.items():\n",
    "    category_map[k] = v.replace(\"b'\",\"\").replace(\"'\",\"\")\n",
    "\n",
    "category_map_reversed = {}\n",
    "for k,v in category_map.items():\n",
    "    category_map_reversed[v] = k\n",
    "    \n",
    "with open(DATA_DIR + \"category_map-\"+str(len(labels.cat.categories))+\".labels\", 'wb') as dumpfile:\n",
    "    pickle.dump(category_map, dumpfile)\n",
    "with open(DATA_DIR + \"category_map_reversed-\"+str(len(labels.cat.categories))+\".labels\", 'wb') as dumpfile:\n",
    "    pickle.dump(category_map_reversed, dumpfile)\n",
    "    \n",
    "category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Shape: (209628, 10)\n",
      "Sampled Features Shape: (200000, 10)\n",
      "Sampled Labels Shape: (200000,)\n",
      "CPU times: user 21.4 ms, sys: 0 ns, total: 21.4 ms\n",
      "Wall time: 20.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Features Shape:', features.shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "sampled_features, _, sampled_labels, _ = train_test_split(features, labels.cat.codes, train_size=200000, random_state = 42)\n",
    "\n",
    "print('Sampled Features Shape:', sampled_features.shape)\n",
    "print('Sampled Labels Shape:', sampled_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  4, 16,  3,  2,  6,  0, 19, 10, 15,  5,  7,  9,  8, 12,  1, 14,\n",
       "       20, 13, 17, 18])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_labels.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (160000, 10)\n",
      "Training Labels Shape: (160000,)\n",
      "Training Instances per class: 7619\n",
      "Testing Features Shape: (40000, 10)\n",
      "Testing Labels Shape: (40000,)\n",
      "Testing Instances per class: 1904\n",
      "CPU times: user 23.7 ms, sys: 0 ns, total: 23.7 ms\n",
      "Wall time: 22.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(sampled_features, sampled_labels, test_size = 0.20, random_state = 42)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Training Instances per class:', math.floor(train_labels.shape[0]/len(sampled_labels.unique())))\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "print('Testing Instances per class:', math.floor(test_labels.shape[0]/len(sampled_labels.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.8 s, sys: 380 ms, total: 42.2 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import lightgbm as lgb\n",
    "model = lgb.LGBMClassifier(n_jobs=4)\n",
    "\n",
    "times = {}\n",
    "times['train'] = {\n",
    "        'wall': {'total': 0, 'instance': 0},\n",
    "        'user': {'total': 0, 'instance': 0},\n",
    "    }\n",
    "times['test'] = {\n",
    "        'wall': {'total': 0, 'instance': 0},\n",
    "        'user': {'total': 0, 'instance': 0},\n",
    "    }\n",
    "\n",
    "start_time, start_resources = timestamp(), resource_usage(RUSAGE_SELF)\n",
    "model.fit(train_features, train_labels)\n",
    "end_resources, end_time = resource_usage(RUSAGE_SELF), timestamp()\n",
    "times['train']['wall']['total'] = timedelta(seconds=end_time - start_time)\n",
    "times['train']['wall']['instance'] = times['train']['wall']['total'] / train_features.shape[1]\n",
    "times['train']['user']['total'] = timedelta(seconds=end_resources.ru_utime - start_resources.ru_utime)\n",
    "times['train']['user']['instance'] = times['train']['user']['total'] / train_features.shape[1]\n",
    "\n",
    "start_time, start_resources = timestamp(), resource_usage(RUSAGE_SELF)\n",
    "model.score(test_features, test_labels)\n",
    "end_resources, end_time = resource_usage(RUSAGE_SELF), timestamp()\n",
    "times['test']['wall']['total'] = timedelta(seconds=end_time - start_time)\n",
    "times['test']['wall']['instance'] = times['test']['wall']['total'] / test_features.shape[1]\n",
    "times['test']['user']['total'] = timedelta(seconds=end_resources.ru_utime - start_resources.ru_utime)\n",
    "times['test']['user']['instance'] = times['test']['user']['total'] / test_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_labels_predicted = model.predict(test_features)\n",
    "print(\"Accuracy:   \", metrics.accuracy_score(test_labels, test_labels_predicted))\n",
    "print(\"Prec. Micro:\", metrics.precision_score(test_labels, test_labels_predicted, average='micro'))\n",
    "print(\"Prec. Macro:\", metrics.precision_score(test_labels, test_labels_predicted, average='macro'))\n",
    "print(\"F1 Micro:   \", metrics.f1_score(test_labels, test_labels_predicted, average='micro'))\n",
    "print(\"F1 Macro:   \", metrics.f1_score(test_labels, test_labels_predicted, average='macro'))\n",
    "\n",
    "scores = {\n",
    "    \"Accuracy\":    metrics.accuracy_score(test_labels, test_labels_predicted),\n",
    "    \"Prec. Micro\": metrics.precision_score(test_labels, test_labels_predicted, average='micro'),\n",
    "    \"Prec. Macro\": metrics.precision_score(test_labels, test_labels_predicted, average='macro'),\n",
    "    \"F1 Micro\":    metrics.f1_score(test_labels, test_labels_predicted, average='micro'),\n",
    "    \"F1 Macro\":    metrics.f1_score(test_labels, test_labels_predicted, average='macro'),\n",
    "}\n",
    "\n",
    "cfmtrx = metrics.confusion_matrix(test_labels, test_labels_predicted)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "g = sns.heatmap(cfmtrx, yticklabels=labels.cat.categories, xticklabels=labels.cat.categories, annot=False, cmap=\"Reds\", linecolor=\"#cfcfcf\", linewidths=0.01)\n",
    "bottom, top = g.get_ylim()\n",
    "g.set_ylim(bottom + 0.5, top - 0.5)\n",
    "left, right = g.get_xlim()\n",
    "g.set_xlim(left, right + 0.5)\n",
    "g.set_title(\"LightGBM\\nFeatures: \" + str(train_features.shape[1]) + \" - Classes: \" + str(len(list(labels.cat.categories))))\n",
    "\n",
    "text = \"Scores\\n\" + '\\n'.join([k+\": \"+ \"{:.3f}\".format(v) for k,v in scores.items()]) \\\n",
    "    + \"\\n\\nTraining\\nInstances: \" + str(train_features.shape[0]) + \"\\nInstances per class: \" + str(math.floor(train_features.shape[0]/len(sampled_labels.unique()))) \\\n",
    "    + \"\\nUser Time: \" + str(times['train']['user']['total']) + \"\\nUser Time per instance: \" + str(times['train']['user']['instance']) \\\n",
    "    + \"\\nWall Time: \" + str(times['train']['wall']['total']) + \"\\nWall Time per instance: \" + str(times['train']['wall']['instance']) \\\n",
    "    + \"\\n\\nTesting\\nInstances: \" + str(test_features.shape[0]) + \"\\nInstances per class: \" + str(math.floor(test_features.shape[0]/len(sampled_labels.unique()))) \\\n",
    "    + \"\\nUser Time: \" + str(times['test']['user']['total']) + \"\\nUser Time per instance: \" + str(times['test']['user']['instance']) \\\n",
    "    + \"\\nWall Time: \" + str(times['test']['wall']['total']) + \"\\nWall Time per instance: \" + str(times['test']['wall']['instance']) \\\n",
    "    + \"\\n\\nModel Configuration\\n\" + str(model)\n",
    "g.text(1.25, 0.95, text, transform=g.transAxes, fontsize=14, verticalalignment='top')\n",
    "plt.tight_layout()\n",
    "plt.savefig(GRAPHICS_DIR + \"LightGBM-Cl\"+ str(len(list(labels.cat.categories))) +\"-TrIn\"+str(train_features.shape[0])+\".\" + FIG_EXTENSION)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(MODELS_DIR + \"lightgbm-Cl\"+ str(len(list(labels.cat.categories))) +\"-TrIn\"+str(train_features.shape[0])+\".scikit\", 'wb') as dumpfile:\n",
    "    pickle.dump(model, dumpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators = 100, random_state = 42, n_jobs=4)\n",
    "\n",
    "times = {}\n",
    "times['train'] = {\n",
    "        'wall': {'total': 0, 'instance': 0},\n",
    "        'user': {'total': 0, 'instance': 0},\n",
    "    }\n",
    "times['test'] = {\n",
    "        'wall': {'total': 0, 'instance': 0},\n",
    "        'user': {'total': 0, 'instance': 0},\n",
    "    }\n",
    "\n",
    "start_time, start_resources = timestamp(), resource_usage(RUSAGE_SELF)\n",
    "model.fit(train_features, train_labels)\n",
    "end_resources, end_time = resource_usage(RUSAGE_SELF), timestamp()\n",
    "times['train']['wall']['total'] = timedelta(seconds=end_time - start_time)\n",
    "times['train']['wall']['instance'] = times['train']['wall']['total'] / train_features.shape[1]\n",
    "times['train']['user']['total'] = timedelta(seconds=end_resources.ru_utime - start_resources.ru_utime)\n",
    "times['train']['user']['instance'] = times['train']['user']['total'] / train_features.shape[1]\n",
    "\n",
    "start_time, start_resources = timestamp(), resource_usage(RUSAGE_SELF)\n",
    "model.score(test_features, test_labels)\n",
    "end_resources, end_time = resource_usage(RUSAGE_SELF), timestamp()\n",
    "times['test']['wall']['total'] = timedelta(seconds=end_time - start_time)\n",
    "times['test']['wall']['instance'] = times['test']['wall']['total'] / test_features.shape[1]\n",
    "times['test']['user']['total'] = timedelta(seconds=end_resources.ru_utime - start_resources.ru_utime)\n",
    "times['test']['user']['instance'] = times['test']['user']['total'] / test_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_labels_predicted = model.predict(test_features)\n",
    "print(\"Accuracy:   \", metrics.accuracy_score(test_labels, test_labels_predicted))\n",
    "print(\"Prec. Micro:\", metrics.precision_score(test_labels, test_labels_predicted, average='micro'))\n",
    "print(\"Prec. Macro:\", metrics.precision_score(test_labels, test_labels_predicted, average='macro'))\n",
    "print(\"F1 Micro:   \", metrics.f1_score(test_labels, test_labels_predicted, average='micro'))\n",
    "print(\"F1 Macro:   \", metrics.f1_score(test_labels, test_labels_predicted, average='macro'))\n",
    "\n",
    "scores = {\n",
    "    \"Accuracy\":    metrics.accuracy_score(test_labels, test_labels_predicted),\n",
    "    \"Prec. Micro\": metrics.precision_score(test_labels, test_labels_predicted, average='micro'),\n",
    "    \"Prec. Macro\": metrics.precision_score(test_labels, test_labels_predicted, average='macro'),\n",
    "    \"F1 Micro\":   metrics.f1_score(test_labels, test_labels_predicted, average='micro'),\n",
    "    \"F1 Macro\":   metrics.f1_score(test_labels, test_labels_predicted, average='macro'),\n",
    "}\n",
    "\n",
    "cfmtrx = metrics.confusion_matrix(test_labels, test_labels_predicted)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "g = sns.heatmap(cfmtrx, yticklabels=labels.cat.categories, xticklabels=labels.cat.categories, annot=False, cmap=\"Reds\", linecolor=\"#cfcfcf\", linewidths=0.01)\n",
    "bottom, top = g.get_ylim()\n",
    "g.set_ylim(bottom + 0.5, top - 0.5)\n",
    "left, right = g.get_xlim()\n",
    "g.set_xlim(left, right + 0.5)\n",
    "g.set_title(\"Random Forest\\nFeatures: \" + str(train_features.shape[1]) + \" - Classes: \" + str(len(list(labels.cat.categories))))\n",
    "\n",
    "text = \"Scores\\n\" + '\\n'.join([k+\": \"+ \"{:.3f}\".format(v) for k,v in scores.items()]) \\\n",
    "    + \"\\n\\nTraining\\nInstances: \" + str(train_features.shape[0]) + \"\\nInstances per class: \" + str(math.floor(train_features.shape[0]/len(sampled_labels.unique()))) \\\n",
    "    + \"\\nUser Time: \" + str(times['train']['user']['total']) + \"\\nUser Time per instance: \" + str(times['train']['user']['instance']) \\\n",
    "    + \"\\nWall Time: \" + str(times['train']['wall']['total']) + \"\\nWall Time per instance: \" + str(times['train']['wall']['instance']) \\\n",
    "    + \"\\n\\nTesting\\nInstances: \" + str(test_features.shape[0]) + \"\\nInstances per class: \" + str(math.floor(test_features.shape[0]/len(sampled_labels.unique()))) \\\n",
    "    + \"\\nUser Time: \" + str(times['test']['user']['total']) + \"\\nUser Time per instance: \" + str(times['test']['user']['instance']) \\\n",
    "    + \"\\nWall Time: \" + str(times['test']['wall']['total']) + \"\\nWall Time per instance: \" + str(times['test']['wall']['instance']) \\\n",
    "    + \"\\n\\nModel Configuration\\n\" + str(model)\n",
    "g.text(1.25, 0.95, text, transform=g.transAxes, fontsize=14, verticalalignment='top')\n",
    "plt.tight_layout()\n",
    "plt.savefig(GRAPHICS_DIR + \"RandomForest-Cl\"+ str(len(list(labels.cat.categories))) +\"-TrIn\"+str(train_features.shape[0])+\".\" + FIG_EXTENSION)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODELS_DIR + \"randomforest-Cl\"+ str(len(list(labels.cat.categories))) +\"-TrIn\"+str(train_features.shape[0])+\".scikit\", 'wb') as dumpfile:\n",
    "    pickle.dump(model, dumpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier(random_state=42, learning_rate=0.01, n_jobs=4)\n",
    "\n",
    "times = {}\n",
    "times['train'] = {\n",
    "        'wall': {'total': 0, 'instance': 0},\n",
    "        'user': {'total': 0, 'instance': 0},\n",
    "    }\n",
    "times['test'] = {\n",
    "        'wall': {'total': 0, 'instance': 0},\n",
    "        'user': {'total': 0, 'instance': 0},\n",
    "    }\n",
    "\n",
    "start_time, start_resources = timestamp(), resource_usage(RUSAGE_SELF)\n",
    "model.fit(train_features, train_labels)\n",
    "end_resources, end_time = resource_usage(RUSAGE_SELF), timestamp()\n",
    "times['train']['wall']['total'] = timedelta(seconds=end_time - start_time)\n",
    "times['train']['wall']['instance'] = times['train']['wall']['total'] / train_features.shape[1]\n",
    "times['train']['user']['total'] = timedelta(seconds=end_resources.ru_utime - start_resources.ru_utime)\n",
    "times['train']['user']['instance'] = times['train']['user']['total'] / train_features.shape[1]\n",
    "\n",
    "start_time, start_resources = timestamp(), resource_usage(RUSAGE_SELF)\n",
    "model.score(test_features, test_labels)\n",
    "end_resources, end_time = resource_usage(RUSAGE_SELF), timestamp()\n",
    "times['test']['wall']['total'] = timedelta(seconds=end_time - start_time)\n",
    "times['test']['wall']['instance'] = times['test']['wall']['total'] / test_features.shape[1]\n",
    "times['test']['user']['total'] = timedelta(seconds=end_resources.ru_utime - start_resources.ru_utime)\n",
    "times['test']['user']['instance'] = times['test']['user']['total'] / test_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_labels_predicted = model.predict(test_features)\n",
    "print(\"Accuracy:   \", metrics.accuracy_score(test_labels, test_labels_predicted))\n",
    "print(\"Prec. Micro:\", metrics.precision_score(test_labels, test_labels_predicted, average='micro'))\n",
    "print(\"Prec. Macro:\", metrics.precision_score(test_labels, test_labels_predicted, average='macro'))\n",
    "print(\"F1 Micro:   \", metrics.f1_score(test_labels, test_labels_predicted, average='micro'))\n",
    "print(\"F1 Macro:   \", metrics.f1_score(test_labels, test_labels_predicted, average='macro'))\n",
    "\n",
    "\n",
    "scores = {\n",
    "    \"Accuracy\":    metrics.accuracy_score(test_labels, test_labels_predicted),\n",
    "    \"Prec. Micro\": metrics.precision_score(test_labels, test_labels_predicted, average='micro'),\n",
    "    \"Prec. Macro\": metrics.precision_score(test_labels, test_labels_predicted, average='macro'),\n",
    "    \"F1 Micro\":   metrics.f1_score(test_labels, test_labels_predicted, average='micro'),\n",
    "    \"F1 Macro\":   metrics.f1_score(test_labels, test_labels_predicted, average='macro'),\n",
    "}\n",
    "\n",
    "cfmtrx = metrics.confusion_matrix(test_labels, test_labels_predicted)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "g = sns.heatmap(cfmtrx, yticklabels=labels.cat.categories, xticklabels=labels.cat.categories, annot=False, cmap=\"Reds\", linecolor=\"#cfcfcf\", linewidths=0.01)\n",
    "bottom, top = g.get_ylim()\n",
    "g.set_ylim(bottom + 0.5, top - 0.5)\n",
    "left, right = g.get_xlim()\n",
    "g.set_xlim(left, right + 0.5)\n",
    "g.set_title(\"XGBoost\\nFeatures: \" + str(train_features.shape[1]) + \" - Classes: \" + str(len(list(labels.cat.categories))))\n",
    "\n",
    "text = \"Scores\\n\" + '\\n'.join([k+\": \"+ \"{:.3f}\".format(v) for k,v in scores.items()]) \\\n",
    "    + \"\\n\\nTraining\\nInstances: \" + str(train_features.shape[0]) + \"\\nInstances per class: \" + str(math.floor(train_features.shape[0]/len(sampled_labels.unique()))) \\\n",
    "    + \"\\nUser Time: \" + str(times['train']['user']['total']) + \"\\nUser Time per instance: \" + str(times['train']['user']['instance']) \\\n",
    "    + \"\\nWall Time: \" + str(times['train']['wall']['total']) + \"\\nWall Time per instance: \" + str(times['train']['wall']['instance']) \\\n",
    "    + \"\\n\\nTesting\\nInstances: \" + str(test_features.shape[0]) + \"\\nInstances per class: \" + str(math.floor(test_features.shape[0]/len(sampled_labels.unique()))) \\\n",
    "    + \"\\nUser Time: \" + str(times['test']['user']['total']) + \"\\nUser Time per instance: \" + str(times['test']['user']['instance']) \\\n",
    "    + \"\\nWall Time: \" + str(times['test']['wall']['total']) + \"\\nWall Time per instance: \" + str(times['test']['wall']['instance']) \\\n",
    "    + \"\\n\\nModel Configuration\\n\" + str(model)\n",
    "g.text(1.25, 0.95, text, transform=g.transAxes, fontsize=14, verticalalignment='top')\n",
    "plt.tight_layout()\n",
    "plt.savefig(GRAPHICS_DIR + \"XGBClassifier-Cl\"+ str(len(list(labels.cat.categories))) +\"-TrIn\"+str(train_features.shape[0])+\".\" + FIG_EXTENSION)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODELS_DIR + \"xgboost-Cl\"+ str(len(list(labels.cat.categories))) +\"-TrIn\"+str(train_features.shape[0])+\".scikit\", 'wb') as dumpfile:\n",
    "    pickle.dump(model, dumpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters = {\n",
    "    'num_leaves': [len(labels.unique()), 50, 75, 85, 100],\n",
    "    'n_estimators': [50, 75, 85, 100, 120, 150, 200],\n",
    "}\n",
    "search = model_selection.GridSearchCV(model, param_grid=parameters, cv=5, n_jobs=1, pre_dispatch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "search.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(search.cv_results_)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.param_max_depth.fillna(value=\"Unlim\", inplace=True)\n",
    "res.param_max_features.fillna(value=\"Auto\", inplace=True)\n",
    "#res.param_n_estimators = \"p_\" + res.param_n_estimators.astype(str)\n",
    "res.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "g = sns.scatterplot(data=res, \n",
    "                    x=res.mean_fit_time, \n",
    "                    y=res.mean_test_score, \n",
    "                    style=res.param_n_estimators, \n",
    "                    #hue=res.param_num_leaves,\n",
    "                    s=100\n",
    "                   )\n",
    "\n",
    "plt.ylim(bottom=0.8, top=1.0)\n",
    "\n",
    "\n",
    "def label_point(x, y, val, ax):\n",
    "    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n",
    "    for i, point in a.iterrows():\n",
    "        ax.text(point['x']+.15, point['y']+.005, str(point['val']))\n",
    "\n",
    "#label_point(res.mean_fit_time, res.mean_test_score, res.rank_test_score, g) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=res, \n",
    "             x_vars=['mean_fit_time', 'mean_test_score', 'param_num_leaves'], \n",
    "             y_vars=['mean_fit_time', 'mean_test_score', 'param_num_leaves'], \n",
    "             hue=\"param_n_estimators\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd.es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get online features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(domain):\n",
    "    import requests,json,psutil,datetime\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    try:\n",
    "        start = datetime.datetime.now()\n",
    "        r = requests.post(\"http://localhost:8080/DGA/domain/features\", data=json.dumps({\"fqdn\": domain}), headers=headers)\n",
    "        #print(r.json())\n",
    "        features = {}\n",
    "        for elem in r.json():\n",
    "            for key in elem.keys():\n",
    "                features[key] = elem[key]\n",
    "                \n",
    "        del(features['class'])\n",
    "        del(features['domain'])\n",
    "        return {\n",
    "            'domain': domain,\n",
    "            'client_time': (datetime.datetime.now()-start).total_seconds() *1000,\n",
    "            'status_code': r.status_code,\n",
    "            'server_time': r.headers['time'],\n",
    "            'features': features\n",
    "        }\n",
    "    except Exception as ex:\n",
    "        return {\n",
    "            'domain': domain,\n",
    "            'exception': ex\n",
    "        }\n",
    "    \n",
    "def classify(domain):\n",
    "    evaluation_features = pd.DataFrame(columns=train_features.columns)\n",
    "    evaluation_features = evaluation_features.append(process(domain)['features'], ignore_index=True).astype('float')\n",
    "    return model.predict(evaluation_features)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_domains = pd.read_csv(DATA_DIR + \"Evaluation.csv\").sample(frac=1, random_state=42).groupby(by=['label']).head(10)\n",
    "for key,values in to_be_replaced.items():\n",
    "    for value in values:\n",
    "        evaluation_domains['label'] = evaluation_domains['label'].str.replace(pat=value, repl=key, regex=False)\n",
    "evaluation_domains['predicted'] = None\n",
    "display(evaluation_domains.info())\n",
    "display(evaluation_domains.sample())\n",
    "display(evaluation_domains.label.unique())\n",
    "evaluation_domains['label_codes'] = np.vectorize(lambda x: category_map_reversed[x])(evaluation_domains['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "times = {}\n",
    "times['test'] = {\n",
    "        'wall': {'total': 0, 'instance': 0},\n",
    "        'user': {'total': 0, 'instance': 0},\n",
    "    }\n",
    "\n",
    "start_time, start_resources = timestamp(), resource_usage(RUSAGE_SELF)\n",
    "evaluation_domains['predicted'] = np.vectorize(lambda domain: category_map[classify(domain)])(evaluation_domains['domain'])\n",
    "end_resources, end_time = resource_usage(RUSAGE_SELF), timestamp()\n",
    "times['test']['wall']['total'] = timedelta(seconds=end_time - start_time)\n",
    "times['test']['wall']['instance'] = times['test']['wall']['total'] / evaluation_domains.shape[0]\n",
    "times['test']['user']['total'] = timedelta(seconds=end_resources.ru_utime - start_resources.ru_utime)\n",
    "times['test']['user']['instance'] = times['test']['user']['total'] / evaluation_domains.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_domains.predicted.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluation_domains['predicted_codes'] = np.vectorize(lambda x: category_map_reversed[x])(evaluation_domains['predicted'])\n",
    "evaluation_domains.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Accuracy:   \", metrics.accuracy_score(evaluation_domains['label_codes'], evaluation_domains['predicted_codes']))\n",
    "print(\"Prec. Micro:\", metrics.precision_score(evaluation_domains['label_codes'], evaluation_domains['predicted_codes'], average='micro'))\n",
    "print(\"Prec. Macro:\", metrics.precision_score(evaluation_domains['label_codes'], evaluation_domains['predicted_codes'], average='macro'))\n",
    "print(\"F1 Micro:   \", metrics.f1_score(evaluation_domains['label_codes'], evaluation_domains['predicted_codes'], average='micro'))\n",
    "print(\"F1 Macro:   \", metrics.f1_score(evaluation_domains['label_codes'], evaluation_domains['predicted_codes'], average='macro'))\n",
    "\n",
    "\n",
    "scores = {\n",
    "    \"Accuracy\":    metrics.accuracy_score(evaluation_domains['label_codes'], evaluation_domains['predicted_codes']),\n",
    "    \"Prec. Micro\": metrics.precision_score(evaluation_domains['label_codes'], evaluation_domains['predicted_codes'], average='micro'),\n",
    "    \"Prec. Macro\": metrics.precision_score(evaluation_domains['label_codes'], evaluation_domains['predicted_codes'], average='macro'),\n",
    "    \"F1 Micro\":   metrics.f1_score(evaluation_domains['label_codes'], evaluation_domains['predicted_codes'], average='micro'),\n",
    "    \"F1 Macro\":   metrics.f1_score(evaluation_domains['label_codes'], evaluation_domains['predicted_codes'], average='macro'),\n",
    "}\n",
    "\n",
    "cfmtrx = metrics.confusion_matrix(evaluation_domains['label_codes'], evaluation_domains['predicted_codes'])\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "g = sns.heatmap(cfmtrx, yticklabels=labels.cat.categories, xticklabels=labels.cat.categories, annot=False, cmap=\"Reds\", linecolor=\"#cfcfcf\", linewidths=0.01)\n",
    "bottom, top = g.get_ylim()\n",
    "g.set_ylim(bottom + 0.5, top - 0.5)\n",
    "left, right = g.get_xlim()\n",
    "g.set_xlim(left, right + 0.5)\n",
    "g.set_title(\"Online Test\\nFeatures: \" + str(train_features.shape[1]) + \" - Classes: \" + str(len(list(labels.cat.categories))))\n",
    "\n",
    "text = \"Scores\\n\" + '\\n'.join([k+\": \"+ \"{:.3f}\".format(v) for k,v in scores.items()]) \\\n",
    "    + \"\\n\\nTesting\\nInstances: \" + str(evaluation_domains.shape[0]) + \"\\nInstances per class: \" + str(math.floor(evaluation_domains.shape[0]/len(sampled_labels.unique()))) \\\n",
    "    + \"\\nUser Time: \" + str(times['test']['user']['total']) + \"\\nUser Time per instance: \" + str(times['test']['user']['instance']) \\\n",
    "    + \"\\nWall Time: \" + str(times['test']['wall']['total']) + \"\\nWall Time per instance: \" + str(times['test']['wall']['instance']) \\\n",
    "    + \"\\n\\nModel Configuration\\n\" + str(model)\n",
    "g.text(1.25, 0.95, text, transform=g.transAxes, fontsize=14, verticalalignment='top')\n",
    "plt.tight_layout()\n",
    "plt.savefig(GRAPHICS_DIR + \"OnlineTest.\" + FIG_EXTENSION)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import lightgbm as lgb\n",
    "model = lgb.LGBMClassifier(num_leaves=31, learning_rate=0.05, n_estimators=20, n_jobs=4)\n",
    "\n",
    "selector = RFE(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "selector = selector.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(selector.support_)\n",
    "sum(selector.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns[selector.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = selector.estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_test_features = selector.transform(test_features)\n",
    "model.score(reduced_test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_labels_predicted = model.predict(reduced_test_features)\n",
    "print(\"Accuracy:   \", metrics.accuracy_score(test_labels, test_labels_predicted))\n",
    "print(\"Prec. Micro:\", metrics.precision_score(test_labels, test_labels_predicted, average='micro'))\n",
    "print(\"Prec. Macro:\", metrics.precision_score(test_labels, test_labels_predicted, average='macro'))\n",
    "print(\"F1 Micro:   \", metrics.f1_score(test_labels, test_labels_predicted, average='micro'))\n",
    "print(\"F1 Macro:   \", metrics.f1_score(test_labels, test_labels_predicted, average='macro'))\n",
    "\n",
    "cfmtrx = metrics.confusion_matrix(test_labels, test_labels_predicted)\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "g = sns.heatmap(cfmtrx, yticklabels=labels.cat.categories, xticklabels=labels.cat.categories, annot=False, cmap=\"Reds\", linecolor=\"#cfcfcf\", linewidths=0.01)\n",
    "bottom, top = g.get_ylim()\n",
    "g.set_ylim(bottom + 0.5, top - 0.5)\n",
    "left, right = g.get_xlim()\n",
    "g.set_xlim(left, right + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_autoencoder(features, labels, enc_layers, dec_layers, layers_text=\"\", epochs=32, batch_size=1000):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import MinMaxScaler    \n",
    "    from keras.models import Model\n",
    "    scaler = MinMaxScaler()\n",
    "    features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
    "    features.describe()\n",
    "    # Split the data into training and testing sets\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels.cat.codes, test_size = 0.20, random_state = 42)\n",
    "\n",
    "    input_layer = Input(shape = (train_features.shape[1], ), name=\"I\")\n",
    "    output_layer = input_layer\n",
    "\n",
    "    for layer in enc_layers:\n",
    "        output_layer = layer(output_layer)\n",
    "\n",
    "    for layer in dec_layers:\n",
    "        output_layer = layer(output_layer)\n",
    "\n",
    "    autoencoder = Model(input = input_layer, output = output_layer, name=\"Autoencoder\")\n",
    "\n",
    "    # CONFIGURE AND TRAIN THE AUTOENCODER\n",
    "    autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')\n",
    "    autoencoder.summary()\n",
    "\n",
    "    autoencoder.fit(train_features, train_features, epochs = epochs, batch_size = batch_size, shuffle = True, validation_data = (test_features, test_features), workers=4)\n",
    "\n",
    "    test = autoencoder.get_layer(\"E_OUT\")(autoencoder.get_layer(\"E3\")(autoencoder.get_layer(\"E2\")(autoencoder.get_layer(\"E1\")(input_layer))))\n",
    "    \n",
    "    encoder_model = Model(input = input_layer, output = test, name=\"Encoder\")\n",
    "    encoded_input = Input(shape = (encoding_dim, ))\n",
    "    encoded_train_features = encoder_model.predict(train_features)\n",
    "    encoded_test_features = encoder_model.predict(test_features)\n",
    "    \n",
    "    import lightgbm as lgb\n",
    "    model = lgb.LGBMClassifier(num_leaves=31, learning_rate=0.05, n_estimators=20, n_jobs=4)\n",
    "    model.fit(encoded_train_features, train_labels)\n",
    "    model.score(encoded_test_features, test_labels)\n",
    "\n",
    "    test_labels_predicted = model.predict(encoded_test_features)\n",
    "    \n",
    "    scores = {\n",
    "        \"Accuracy\":    metrics.accuracy_score(test_labels, test_labels_predicted),\n",
    "        \"Prec. Micro\": metrics.precision_score(test_labels, test_labels_predicted, average='micro'),\n",
    "        \"Prec. Macro\": metrics.precision_score(test_labels, test_labels_predicted, average='macro'),\n",
    "        \"F1 Micro\":   metrics.f1_score(test_labels, test_labels_predicted, average='micro'),\n",
    "        \"F1 Macro\":   metrics.f1_score(test_labels, test_labels_predicted, average='macro'),\n",
    "    }\n",
    "    \n",
    "    cfmtrx = metrics.confusion_matrix(test_labels, test_labels_predicted)\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    g = sns.heatmap(cfmtrx, yticklabels=labels.cat.categories, xticklabels=labels.cat.categories, annot=False, cmap=\"Reds\", linecolor=\"#cfcfcf\", linewidths=0.01)\n",
    "    bottom, top = g.get_ylim()\n",
    "    g.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    left, right = g.get_xlim()\n",
    "    g.set_xlim(left, right + 0.5)\n",
    "    g.set_title(\"LightGBM + Autoencoder (\" + str(encoded_train_features.shape[1]) + \"neurons)\")\n",
    "\n",
    "\n",
    "    text = \"Scores\\n\" + '\\n'.join([k+\": \"+ \"{:.3f}\".format(v) for k,v in scores.items()]) + \"\\n\\nAutoencoder Model:\\n\" + layers_text\n",
    "    g.text(0.55, 0.95, text, transform=g.transAxes, fontsize=14, verticalalignment='top')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "\n",
    "layers = \"\"\n",
    "\n",
    "# DEFINE THE DIMENSION OF ENCODER\n",
    "encoding_dim = 4*len(labels.unique())\n",
    "\n",
    "# DEFINE THE ENCODER LAYERS\n",
    "enc_layers = []\n",
    "layers += \"Input(\"+str(train_features.shape[1])+\")\\n\"\n",
    "enc_layers.append(Dense(256, activation = 'relu', name=\"E1\"))\n",
    "layers += \"E1: Dense(256, 'relu')\\n\"\n",
    "enc_layers.append(Dense(512, activation = 'relu', name=\"E2\"))\n",
    "layers += \"E2: Dense(128, 'relu')\\n\"\n",
    "enc_layers.append(Dense(1024, activation = 'relu', name=\"E3\"))\n",
    "layers += \"E3: Dense(64, 'relu')\\n\"\n",
    "enc_layers.append(Dense(encoding_dim, activation = 'relu', name=\"E_OUT\"))\n",
    "layers += \"E_OUT: Dense(\"+str(encoding_dim)+\", 'relu')\\n\"\n",
    "layers += \"---------------\\n\"\n",
    "\n",
    "dec_layers = []\n",
    "# DEFINE THE DECODER LAYERS\n",
    "enc_layers.append(Dense(1024, activation = 'relu', name=\"D1\"))\n",
    "layers += \"D1: Dense(64, 'relu')\\n\"\n",
    "enc_layers.append(Dense(512, activation = 'relu', name=\"D2\"))\n",
    "layers += \"D2: Dense(128, 'relu')\\n\"\n",
    "enc_layers.append(Dense(256, activation = 'relu', name=\"D3\"))\n",
    "layers += \"D3: Dense(256, 'relu')\\n\"\n",
    "enc_layers.append(Dense(train_features.shape[1], activation = 'sigmoid', name=\"D_OUT\"))\n",
    "layers += \"D_OUT: Dense(\"+str(train_features.shape[1])+\", 'sigmoid')\\n\"\n",
    "    \n",
    "test_autoencoder(features, labels, enc_layers, dec_layers, layers_text = layers, epochs=32, batch_size=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
